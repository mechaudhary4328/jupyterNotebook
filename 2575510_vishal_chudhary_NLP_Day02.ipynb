{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01699092",
   "metadata": {},
   "source": [
    "# 1. What is the purpose of text preprocessing in NLP, and why is it essential before analysis?\n",
    "2. Describe tokenization in NLP and explain its significance in text processing.\n",
    "3. What are the differences between stemming and lemmatization in NLP? When would you \n",
    "choose one over the other?\n",
    "4. Explain the concept of stop words and their role in text preprocessing. How do they impact \n",
    "NLP tasks?\n",
    "5. How does the process of removing punctuation contribute to text preprocessing in NLP? \n",
    "What are its benefits?\n",
    "6. Discuss the importance of lowercase conversion in text preprocessing. Why is it a \n",
    "common step in NLP tasks?\n",
    "7. Explain the term \"vectorization\" concerning text data. How does techniques like \n",
    "CountVectorizer contribute to text preprocessing in NLP?\n",
    "8. Describe the concept of normalization in NLP. Provide examples of normalization \n",
    "techniques used in text preprocessing.\n",
    "Note: Consider the text. It may be a file or prompted inputs.\n",
    " Python code is mandate for possible Questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48daa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "Text preprocessing is a crucial step in natural language processing (NLP) that involves cleaning and transforming raw text data into a format that is suitable for analysis. There are several reasons why text preprocessing is essential before performing any NLP tasks:\n",
    "\n",
    "Noise Reduction:\n",
    "\n",
    "Raw text data often contains noise, such as special characters, HTML tags, and irrelevant symbols. Preprocessing helps remove or replace these elements, reducing noise and ensuring that the data is clean.\n",
    "Tokenization:\n",
    "\n",
    "Tokenization involves breaking down text into smaller units, such as words or subwords. This step is essential for most NLP tasks as it converts continuous text into discrete tokens, making it easier to analyze and work with.\n",
    "Lowercasing:\n",
    "\n",
    "Standardizing the case of words by converting them to lowercase helps ensure consistency in the analysis. This is important, especially for tasks like text classification or sentiment analysis, where the case of words may not carry significant meaning.\n",
    "Stopword Removal:\n",
    "\n",
    "Stopwords are common words (e.g., \"and,\" \"the,\" \"is\") that do not contribute much to the meaning of a sentence. Removing stopwords during preprocessing can help reduce the dimensionality of the data and focus on more meaningful words.\n",
    "Stemming and Lemmatization:\n",
    "\n",
    "These techniques involve reducing words to their base or root form. Stemming removes prefixes and suffixes, while lemmatization considers the context of the word and reduces it to its base form. These processes help in normalizing words and reducing variations.\n",
    "Handling Contractions and Abbreviations:\n",
    "\n",
    "Text preprocessing helps in expanding contractions (e.g., \"don't\" to \"do not\") and handling abbreviations. This ensures that the text is consistent and can be properly understood by models.\n",
    "Handling Missing or Noisy Data:\n",
    "\n",
    "Text data might contain missing or noisy information. Preprocessing helps in addressing these issues, either by imputing missing values or by removing instances with unreliable information.\n",
    "Normalization:\n",
    "\n",
    "Text data may have variations in spelling, such as \"color\" vs. \"colour\" or \"organization\" vs. \"organisation.\" Normalization ensures consistent representation of words, reducing the impact of spelling variations.\n",
    "Feature Extraction:\n",
    "\n",
    "Depending on the analysis, additional features may be extracted during preprocessing, such as n-grams, part-of-speech tags, or named entities. These features can provide valuable information for certain NLP tasks.\n",
    "By performing these preprocessing steps, the data becomes more manageable, consistent, and suitable for analysis. Clean and standardized text data contribute to the effectiveness of NLP models and algorithms, leading to more accurate and reliable results in tasks such as text classification, sentiment analysis, machine translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f1eff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 16) (3024963269.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    Stemming is often preferred in applications where speed and simplicity are crucial, and where the recognition of valid words is not as important. It's commonly used in information retrieval and search engines.\u001b[0m\n\u001b[0m                                                                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 16)\n"
     ]
    }
   ],
   "source": [
    "3:\n",
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base or root form, but they have distinct differences in terms of their approaches and outcomes.\n",
    "\n",
    "Stemming:\n",
    "Approach:\n",
    "\n",
    "Stemming involves removing prefixes or suffixes from words to obtain their root form. The goal is to reduce words to a common base, even if the resulting \"stem\" is not a valid word itself.\n",
    "Speed:\n",
    "\n",
    "Stemming is generally faster than lemmatization because it relies on heuristic rules without considering the context of the word.\n",
    "Results:\n",
    "\n",
    "Stemming may result in the reduction of a word to a root that is not an actual word. For example, \"running\" might be stemmed to \"run,\" which is a valid word but not the base form in the linguistic sense.\n",
    "Use Cases:\n",
    "\n",
    "Stemming is often preferred in applications where speed and simplicity are crucial, and where the recognition of valid words is not as important. It's commonly used in information retrieval and search engines.\n",
    "Lemmatization:\n",
    "Approach:\n",
    "\n",
    "Lemmatization, on the other hand, involves reducing words to their base or dictionary form (known as the lemma) by considering the context and meaning of the word. It often requires access to a lexicon or dictionary.\n",
    "Accuracy:\n",
    "\n",
    "Lemmatization tends to be more accurate than stemming because it considers the meaning of words and ensures that the resulting lemma is a valid word.\n",
    "Results:\n",
    "\n",
    "Lemmatization provides linguistically meaningful base forms. For example, \"better\" might be lemmatized to \"good,\" taking into account the context of the word.\n",
    "Use Cases:\n",
    "\n",
    "Lemmatization is preferred in applications where linguistic accuracy is crucial, such as in machine translation, question-answering systems, and sentiment analysis where understanding the actual meaning of words is important.\n",
    "When to Choose One Over the Other:\n",
    "Stemming:\n",
    "\n",
    "Choose stemming when you need a fast and lightweight method for text normalization, and when the recognition of valid words is not critical. It's suitable for applications like information retrieval.\n",
    "Lemmatization:\n",
    "\n",
    "Choose lemmatization when linguistic accuracy is important, and you want to obtain valid words with their base forms. It's suitable for applications where a deeper understanding of the text is required, such as in sentiment analysis or machine translation.\n",
    "In summary, the choice between stemming and lemmatization depends on the specific requirements of the NLP task at hand. If speed and simplicity are priorities, stemming may be more appropriate. If linguistic accuracy and meaningful base forms are crucial, lemmatization is the preferred choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "Tokenization is a fundamental process in natural language processing (NLP) that involves breaking down a text into smaller units, typically words or subwords. The resulting units, called tokens, serve as the basic building blocks for further analysis. Tokenization is a crucial step in text processing for several reasons:\n",
    "\n",
    "1. Text Segmentation:\n",
    "Tokenization divides a continuous stream of text into individual units, making it more manageable for analysis. It helps create a structured representation of the text by breaking it into discrete elements.\n",
    "2. Feature Extraction:\n",
    "Tokens serve as features in NLP models. By representing text as a sequence of tokens, it becomes possible to extract features such as word frequencies, n-grams, and other patterns, which are essential for various machine learning tasks.\n",
    "3. Text Cleaning:\n",
    "Tokenization is often accompanied by text cleaning steps. It helps identify and handle issues like punctuation, special characters, and formatting, ensuring that the data is prepared for analysis.\n",
    "4. Language Understanding:\n",
    "Tokens provide the basic units for understanding the structure and meaning of a language. Analyzing text at the token level enables NLP models to comprehend the syntactic and semantic relationships between words.\n",
    "5. Word-level Analysis:\n",
    "Tokenization allows for word-level analysis, making it possible to count word frequencies, identify key terms, and perform statistical analyses. This is valuable in tasks such as text classification, sentiment analysis, and information retrieval.\n",
    "6. Sequence Modeling:\n",
    "In sequence-based tasks like machine translation or text generation, tokenization is crucial for creating input sequences. Each token becomes a step in the sequence, and the relationships between tokens are learned by the model.\n",
    "7. Text Representation:\n",
    "Tokenization plays a key role in representing text for machine learning models. Bag-of-words models, word embeddings (like Word2Vec or GloVe), and other representations rely on tokenized text as input.\n",
    "8. Preprocessing for Downstream Tasks:\n",
    "Many NLP tasks, such as part-of-speech tagging, named entity recognition, and sentiment analysis, require tokenized input. Tokenization is often the first step in preparing data for these downstream tasks.\n",
    "9. Efficient Storage and Processing:\n",
    "Tokenization reduces the dimensionality of the data and enables more efficient storage and processing. It simplifies the representation of text, making it easier to handle in computational tasks.\n",
    "10. Multi-modal Data Integration:\n",
    "In the context of multi-modal data (e.g., combining text and images), tokenization is a crucial step for segregating and analyzing the textual component.\n",
    "In summary, tokenization is a foundational step in NLP that facilitates the analysis, understanding, and representation of text data. It is a prerequisite for a wide range of NLP tasks and serves as the basis for building effective and meaningful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c875c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    "Stop words are common words that are often removed from text during the preprocessing stage in natural language processing (NLP). These words are generally the most frequently occurring words in a language but typically do not carry significant meaning on their own. Examples of stop words in English include \"and,\" \"the,\" \"is,\" \"in,\" and \"it.\" The concept of stop words and their role in text preprocessing are crucial for various NLP tasks. Here's a breakdown of their significance:\n",
    "\n",
    "Role of Stop Words in Text Preprocessing:\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Removing stop words helps reduce the dimensionality of the data. Since stop words are often very frequent, keeping them in the analysis may lead to unnecessarily high-dimensional feature spaces, making it harder to discern meaningful patterns.\n",
    "Focus on Content Words:\n",
    "\n",
    "By eliminating stop words, the analysis can focus more on content words, which carry the primary meaning of the text. This can improve the efficiency of algorithms and models by emphasizing words that contribute more to the context.\n",
    "Improved Processing Speed:\n",
    "\n",
    "Excluding stop words can lead to faster processing times in NLP tasks. Since stop words are prevalent and do not contribute much to the understanding of the text, removing them can speed up computations.\n",
    "Memory Efficiency:\n",
    "\n",
    "The removal of stop words contributes to memory efficiency, as the processed data is smaller and requires less storage. This is particularly important when working with large datasets.\n",
    "Enhanced Interpretability:\n",
    "\n",
    "Analyzing text without stop words often results in more interpretable and meaningful results. The focus on content words provides insights into the key themes and topics of the text.\n",
    "Improved Performance in Certain Tasks:\n",
    "\n",
    "For some NLP tasks like sentiment analysis or document classification, stop words may not carry much sentiment or class-specific information. Removing them can enhance the performance of models by emphasizing more meaningful terms.\n",
    "Impact on NLP Tasks:\n",
    "Information Retrieval:\n",
    "\n",
    "In tasks related to information retrieval, removing stop words can improve the accuracy of searches by focusing on keywords that are more indicative of the document's content.\n",
    "Text Classification:\n",
    "\n",
    "In text classification tasks, such as spam detection or sentiment analysis, stop words may not contribute much to determining the category. Removing them can lead to better feature representation and classification accuracy.\n",
    "Topic Modeling:\n",
    "\n",
    "In topic modeling tasks like Latent Dirichlet Allocation (LDA), excluding stop words can help identify more meaningful topics by emphasizing content words that distinguish between topics.\n",
    "Search Engine Optimization (SEO):\n",
    "\n",
    "In the context of SEO, removing stop words from web content can improve the relevance of search results and enhance the overall user experience.\n",
    "While stop words are often removed, it's essential to note that the decision to include or exclude them depends on the specific requirements of the NLP task. In certain contexts, such as language modeling or certain information extraction tasks, stop words may be retained to preserve the syntactic structure of the text. The choice to remove or retain stop words should be made based on the goals and characteristics of the particular NLP application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ccfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    "    \n",
    "Removing punctuation is a common step in text preprocessing for natural language processing (NLP). Punctuation marks, such as periods, commas, quotation marks, and other symbols, are often non-informative and can introduce noise into the data. The process of removing punctuation contributes to cleaner and more standardized text data, offering several benefits in NLP tasks:\n",
    "\n",
    "1. Noise Reduction:\n",
    "Punctuation marks often do not carry significant meaning by themselves. Removing them helps reduce noise in the text data, allowing NLP models to focus on more meaningful content words.\n",
    "2. Consistency in Analysis:\n",
    "Standardizing the text by removing punctuation ensures consistency in the analysis. For example, \"word\" and \"word!\" might represent the same concept, and removing punctuation helps treat them as such.\n",
    "3. Tokenization:\n",
    "Punctuation marks can interfere with the tokenization process, making it challenging to extract individual words. Removing punctuation facilitates the tokenization of text, creating a cleaner sequence of tokens for analysis.\n",
    "4. Efficient Feature Extraction:\n",
    "Feature extraction methods, such as bag-of-words or TF-IDF (Term Frequency-Inverse Document Frequency), rely on the presence of distinct words. Punctuation, if included, may result in additional and unnecessary features. Removing punctuation contributes to more efficient and meaningful feature extraction.\n",
    "5. Improved Text Representation:\n",
    "Punctuation can introduce variations in text representation that may not contribute to the semantics of the language. Removing punctuation helps in creating more consistent and representative text representations.\n",
    "6. Preventing Sparsity in Data:\n",
    "Including punctuation in the analysis may lead to sparse data representations, especially when using methods like bag-of-words. Removing punctuation helps avoid unnecessary sparsity and ensures a more compact and meaningful representation.\n",
    "7. Language Model Training:\n",
    "When training language models, removing punctuation helps the model focus on understanding the context and meaning of words without being influenced by punctuation marks.\n",
    "8. Improved Search and Retrieval:\n",
    "In information retrieval tasks, removing punctuation can improve the accuracy of searches by ensuring that queries and documents are matched based on content words rather than punctuations.\n",
    "9. Better Token Matching:\n",
    "In certain tasks, such as named entity recognition, removing punctuation ensures better matching of tokens, making it easier to identify entities without interference from punctuation marks.\n",
    "10. Text Normalization:\n",
    "Removing punctuation is a part of text normalization, ensuring that different representations of the same text are treated uniformly. This is important for creating consistent and standardized datasets.\n",
    "In summary, removing punctuation during text preprocessing in NLP contributes to cleaner, more consistent, and more meaningful data. It supports various NLP tasks by enhancing tokenization, feature extraction, and overall text representation. The benefits include improved efficiency, reduced noise, and better performance of NLP models across a range of applications, including text classification, sentiment analysis, machine translation, and more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    "1. Normalization:\n",
    "Lowercasing helps in normalizing the text by ensuring a consistent representation. This is crucial because words with different cases (e.g., \"Word\" and \"word\") should be treated as identical, especially in tasks where the case doesn't carry specific semantic meaning.\n",
    "2. Uniformity in Features:\n",
    "In many NLP tasks, words are used as features for analysis. Converting text to lowercase ensures that the same word is represented consistently, reducing the dimensionality of the feature space and improving the efficiency of algorithms.\n",
    "3. Enhanced Tokenization:\n",
    "Lowercasing facilitates tokenization, the process of breaking down text into smaller units (tokens). Tokenization is a fundamental step in NLP, and it becomes simpler and more accurate when the text is in lowercase.\n",
    "4. Improved Matching:\n",
    "Lowercasing ensures better matching of words in tasks such as information retrieval, search engines, and named entity recognition. It helps avoid discrepancies in matching due to variations in letter case.\n",
    "5. Consistent Word Embeddings:\n",
    "Word embeddings, such as Word2Vec or GloVe, are often trained on lowercase text. Converting text to lowercase ensures compatibility with pre-trained embeddings, promoting consistency in representations.\n",
    "6. Case-Insensitive Matching:\n",
    "In certain NLP tasks, case-insensitive matching is desirable. For example, in text classification or sentiment analysis, the sentiment conveyed by a sentence may not change based on the case of the words.\n",
    "7. Improves Model Generalization:\n",
    "Lowercasing contributes to better generalization of machine learning models. Models are less likely to overfit to specific cases, and the knowledge gained from lowercase text can be more broadly applicable.\n",
    "8. Consistency in Text Data:\n",
    "Ensuring that all text is in lowercase promotes overall consistency in the dataset. This consistency is beneficial for creating clean, standardized datasets that yield more reliable results across different NLP tasks.\n",
    "9. Ease of Comparison:\n",
    "Lowercasing simplifies the comparison of text data. It makes it easier to check for equality or similarity between words, phrases, or documents without being concerned about variations in letter case.\n",
    "10. Compatibility with Databases and Libraries:\n",
    "Lowercasing aligns well with the conventions of many databases and NLP libraries. It ensures compatibility with existing tools and resources, making it easier to integrate text data into various NLP workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b35cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "7:\n",
    "    \n",
    "Word Frequency Representation:\n",
    "\n",
    "CountVectorizer converts each document in the corpus into a vector, where each element represents the frequency of a particular word in that document. This representation captures the distribution of words and their frequencies in the text.\n",
    "Sparse Matrix Representation:\n",
    "\n",
    "The result of CountVectorizer is often a sparse matrix, where most entries are zero. This sparse matrix efficiently represents the text data, saving memory and computational resources.\n",
    "Normalization:\n",
    "\n",
    "CountVectorizer can be configured to normalize the word frequencies, taking into account the length of the documents. This is useful for comparing documents of different lengths.\n",
    "Vocabulary Size Reduction:\n",
    "\n",
    "By setting parameters like maximum and minimum document frequency, CountVectorizer allows for the reduction of the vocabulary size. This can help remove very common or very rare words that may not contribute much to the analysis.\n",
    "Compatibility with Machine Learning Models:\n",
    "\n",
    "The numerical representation produced by CountVectorizer is compatible with a wide range of machine learning models, such as linear models, decision trees, and support vector machines.\n",
    "\n",
    "Here's an example of using CountVectorizer in Python:\n",
    " from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer instance\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense array for better visualization\n",
    "dense_array = X.toarray()\n",
    "\n",
    "# Display the feature names and the resulting matrix\n",
    "print(\"Feature names:\", feature_names)\n",
    "print(\"Vectorized matrix:\")\n",
    "print(dense_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8:\n",
    "    \n",
    "Lowercasing:\n",
    "\n",
    "Converting all characters in the text to lowercase. This ensures case consistency and simplifies further processing.\n",
    "Stemming:\n",
    "\n",
    "Reducing words to their root or base form by removing prefixes or suffixes. For example, \"running\" becomes \"run.\"\n",
    "Lemmatization:\n",
    "\n",
    "Similar to stemming but involves reducing words to their base or dictionary form (lemma). For example, \"better\" becomes \"good.\"\n",
    "Removing Accents/Diacritics:\n",
    "\n",
    "Replacing accented characters with their non-accented counterparts. For example, converting \"résumé\" to \"resume.\"\n",
    "Removing Special Characters and Punctuation:\n",
    "\n",
    "Eliminating non-alphanumeric characters and punctuation marks from the text.\n",
    "Handling Numbers:\n",
    "\n",
    "Standardizing the representation of numbers. For example, converting \"3.14\" to \"3.1416\" or replacing numbers with a generic token like \"<NUM>\".\n",
    "Removing Stopwords:\n",
    "\n",
    "Eliminating common words that do not carry much meaning, such as \"the,\" \"and,\" \"is.\"\n",
    "Here's an example of text normalization in Python using some of these techniques:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Sample text\n",
    "text = \"Normalization is a crucial step in NLP. It involves converting text to lowercase, removing punctuation, and handling numbers.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Lowercasing\n",
    "lowercased_tokens = [token.lower() for token in tokens]\n",
    "\n",
    "# Removing punctuation and special characters\n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in lowercased_tokens]\n",
    "\n",
    "# Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in cleaned_tokens if token not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "stemmed_tokens = [porter_stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "# Display the results\n",
    "print(\"Original text:\", text)\n",
    "print(\"Normalized tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45cf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d47397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40583b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db2d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bea463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f205d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d47ff3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
