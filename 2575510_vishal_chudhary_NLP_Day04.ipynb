{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c469eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/anubhavshroti/Documents/Anaconda/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578314f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data] Downloading package wordnet2022 to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet2022.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import wsd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from spacy.cli import download\n",
    "from spacy import load\n",
    "import warnings\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('wordnet2022')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7159425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list: ['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words\n",
    "valid_words = nltk.corpus.words.words()\n",
    "\n",
    "# Print the first 20 words in the list\n",
    "print(\"First 20 words in the list:\", valid_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f46dbf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "aa\n",
      "aal\n",
      "aalii\n",
      "aam\n",
      "aani\n",
      "aardvark\n",
      "aardwolf\n",
      "aaron\n",
      "aaronic\n",
      "aaronical\n",
      "aaronite\n",
      "aaronitic\n",
      "aaru\n",
      "ab\n",
      "aba\n",
      "ababdeh\n",
      "ababua\n",
      "abac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "for word in valid_words[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06c72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "aa\n",
      "aal\n",
      "aalii\n",
      "aam\n",
      "aani\n",
      "aardvark\n",
      "aardwolf\n",
      "aaron\n",
      "aaronic\n",
      "aaronical\n",
      "aaronite\n",
      "aaronitic\n",
      "aaru\n",
      "ab\n",
      "aba\n",
      "ababdeh\n",
      "ababua\n",
      "abac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Print the first 20 words in the list with normalized casing\n",
    "for word in valid_words[:20]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da6c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the unique list with normalized casing: ['lutetian', 'excess', 'sussultorial', 'mormyrus', 'monatomism', 'nicotinize', 'pylar', 'abrader', 'nodulose', 'preconditioned', 'sent', 'pagurid', 'resiner', 'superether', 'labroid', 'pyrophosphorous', 'undefeat', 'gypsywort', 'jungermannia', 'amethodically']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a unique list after normalizing casing\n",
    "unique_valid_words = list(set(valid_words))\n",
    "\n",
    "# Print the first 20 words in the unique list\n",
    "print(\"First 20 words in the unique list with normalized casing:\", unique_valid_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e4809a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the final list of stop words: ['?', '+', 'needn', 'haven', 'both', 'most', 'why', 'y', 't', 'wouldn', 'where', 'because', 'to', 'myself', 'such', 'should', '<', ':', 'having', 'can']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "# Download the NLTK stop words if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the list of stop words from NLTK\n",
    "stop_words_nltk = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Get all punctuations from the string module\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "# Create the final list of stop words by combining the two sets\n",
    "stop_words_final = stop_words_nltk.union(punctuations)\n",
    "\n",
    "# Print the first 20 words in the final list\n",
    "print(\"First 20 words in the final list of stop words:\", list(stop_words_final)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7441966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct word for 'committee': committee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "# Example: Get the correct word for 'committee'\n",
    "# Define or load your list of valid words\n",
    "valid_words_list = [\"example\", \"committee\", \"other\", \"words\", \"valid\", \"list\", \"of\", \"words\"]  # Add your list of valid words here\n",
    "\n",
    "correct_word_committee = get_correct_term(\"committee\", valid_words_list)\n",
    "print(\"Correct word for 'committee':\", correct_word_committee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d6a4e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'example' is in the valid words list.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the words corpus if not already downloaded\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid English words and normalize the casing\n",
    "valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "\n",
    "# Create a set from the list of valid words for faster lookup\n",
    "valid_words_set = set(valid_words_list)\n",
    "\n",
    "# Example: Check if a word is in the valid words set\n",
    "word_to_check = 'example'\n",
    "if word_to_check in valid_words_set:\n",
    "    print(f\"'{word_to_check}' is in the valid words list.\")\n",
    "else:\n",
    "    print(f\"'{word_to_check}' is not in the valid words list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fc9e101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: the new abactor is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download the 'punkt' resource if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f95e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anubhavshroti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: The new abacos is great\n",
      "Corrected Sentence: the new abactor is great\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Set the NLTK data directory\n",
    "nltk.data.path.append(\"/path/to/nltk_data\")\n",
    "\n",
    "# Download the 'punkt' resource if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "def get_valid_words_set():\n",
    "    # Get the list of valid English words and normalize the casing\n",
    "    valid_words_list = [word.lower() for word in nltk.corpus.words.words()]\n",
    "    # Create a set from the list of valid words for faster lookup\n",
    "    return set(valid_words_list)\n",
    "\n",
    "def get_correct_term(target_term, valid_words_list):\n",
    "    # Use the first 20,000 entries in the valid word list\n",
    "    valid_words_list = valid_words_list[:20000]\n",
    "\n",
    "    # Store the edit distances in a dictionary\n",
    "    edit_distances = {word: edit_distance(target_term, word) for word in valid_words_list}\n",
    "\n",
    "    # Sort the dictionary in ascending order of edit distances\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the first entry in the sorted result (minimum edit distance)\n",
    "    return sorted_distances[0][0]\n",
    "\n",
    "def correct_spelling(input_sentence, valid_words_set):\n",
    "    # Tokenize the input sentence after making all terms lowercase\n",
    "    tokenized_sentence = nltk.word_tokenize(input_sentence.lower())\n",
    "\n",
    "    # Correct spelling for each term in the tokenized sentence\n",
    "    corrected_sentence = [term if term in valid_words_set else get_correct_term(term, list(valid_words_set)) for term in tokenized_sentence]\n",
    "\n",
    "    # Return the joined string as output\n",
    "    return ' '.join(corrected_sentence)\n",
    "\n",
    "# Test the function with the input sentence\n",
    "input_sentence = \"The new abacos is great\"\n",
    "valid_words_set = get_valid_words_set()\n",
    "corrected_sentence = correct_spelling(input_sentence, valid_words_set)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ddd93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
